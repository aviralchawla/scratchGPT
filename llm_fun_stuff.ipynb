{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13769de90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from scratchGPT.utils import Tokenizer, Dataset\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data as full text ------>\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "Data as tokenized integers ------>\n",
      "[31, 56, 3, 24, 62, 4, 32, 56, 62, 56, 39, 51, 12, 17, 7, 30, 51, 33, 19, 3, 51, 4, 0, 51, 4, 14, 3, 19, 58, 51, 51, 40, 4, 43, 12, 46, 4, 33, 1, 3, 62, 5, 51, 3, 18, 4, 5, 51, 43, 3, 4, 64, 51, 4, 24, 14, 51, 43, 25, 41, 7, 7, 42, 28, 28, 17, 7, 57, 14, 51, 43, 25, 18, 4, 24, 14, 51, 43, 25, 41, 7, 7, 31, 56, 3, 24, 62, 4, 32, 56, 62, 56, 39, 51, 12, 17, 7, 26, 19, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text)\n",
    "print('Data as full text ------>')\n",
    "print(text[:100])\n",
    "print('Data as tokenized integers ------>')\n",
    "print(tokenizer.encode(text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model parameters\n",
    "n = 20 # context size\n",
    "v = len(tokenizer) # vocab size\n",
    "d = 64 # embedding dimension\n",
    "a = 4 # number of attention heads\n",
    "d_k = d // a\n",
    "d_v = d_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "T = torch.tensor(tokenizer.encode(text[:n])) # tokenized text\n",
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 64])\n"
     ]
    }
   ],
   "source": [
    "E = torch.randn(v, d) # embedding matrix\n",
    "print(E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x: torch.Tensor, gamma=1, beta=0):\n",
    "    return ((x - x.mean()) / x.std()) * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64])\n"
     ]
    }
   ],
   "source": [
    "X = E[T]\n",
    "print(X.shape) # n x d\n",
    "X = layer_norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "# query, key, value\n",
    "W_Q = [torch.randn(d, d_k) for _ in range(a)]\n",
    "W_K = [torch.randn(d, d_k) for _ in range(a)]\n",
    "W_V = [torch.randn(d, d_v) for _ in range(a)]\n",
    "\n",
    "print(W_Q[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [X @ W_Q[i] for i in range(a)]\n",
    "K = [X @ W_K[i] for i in range(a)]\n",
    "V = [X @ W_V[i] for i in range(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_querykey(Q, K):\n",
    "    o = Q @ K.T\n",
    "    masked = torch.tril(o)\n",
    "    masked = torch.masked_fill(masked, masked == 0, float('-inf'))\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n"
     ]
    }
   ],
   "source": [
    "Ma = [masked_querykey(Q[i], K[i]) for i in range(a)]\n",
    "Ma = [m/np.sqrt(d_k) for m in Ma]\n",
    "print(Ma[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64])\n"
     ]
    }
   ],
   "source": [
    "SoftMa = [softmax(m) for m in Ma]\n",
    "H = torch.concat([SoftMa[i] @ V[i] for i in range(a)], dim=1)\n",
    "print(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64])\n"
     ]
    }
   ],
   "source": [
    "W_O = torch.randn(d, d)\n",
    "O = H @ W_O\n",
    "O = X + O\n",
    "print(O.shape)\n",
    "\n",
    "O = layer_norm(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward\n",
    "W_1 = torch.randn(d, d)\n",
    "W_2 = torch.randn(d, d)\n",
    "b_1 = torch.randn(d)\n",
    "b_2 = torch.randn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: torch.Tensor):\n",
    "    return torch.max(x, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 64])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = relu(O @ W_1 + b_1) @ W_2 + b_2\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 64])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = O + F\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unembedd = output[-1] @ E.T\n",
    "unembedd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = softmax(unembedd)\n",
    "probabilities.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratchGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
